{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import cv2\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Breakout-v0'\n",
    "\n",
    "num_stacked_frames = 4\n",
    "replay_memory_size = 250000\n",
    "min_replay_size_to_update = 25000\n",
    "lr = 0.001\n",
    "gamma = 0.97\n",
    "minibatch_size = 32\n",
    "steps_rollout = 16\n",
    "start_eps = 1\n",
    "final_eps = 0.1\n",
    "final_eps_frame = 1000000\n",
    "total_steps = 5000000\n",
    "target_net_update = 625\n",
    "save_model_steps = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"gpu\")\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Atari_Wrapper(gym.Wrapper):\n",
    "    \n",
    "    def __init__(self, env, env_name, k, dsize=(84,84)):\n",
    "        super(Atari_Wrapper, self).__init__(env)\n",
    "        self.dsize = dsize\n",
    "        self.k = k\n",
    "        self.frame_cutout_h = (31,-16)\n",
    "        self.frame_cutout_w = (7,-7)\n",
    "        \n",
    "    def reset(self):\n",
    "    \n",
    "        self.Return = 0\n",
    "        self.last_life_count = 0\n",
    "        ob = self.env.reset()\n",
    "        ob = self.preprocess_observation(ob)\n",
    "        self.frame_stack = np.stack([ob for i in range(self.k)])\n",
    "        \n",
    "        return self.frame_stack\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        \n",
    "        reward = 0\n",
    "        done = False\n",
    "        frames = []\n",
    "        for i in range(self.k):\n",
    "            ob, r, d, info = self.env.step(action)\n",
    "            ob = self.preprocess_observation(ob)\n",
    "            frames.append(ob)\n",
    "            reward += r\n",
    "            if d:\n",
    "                done = True\n",
    "                break\n",
    "                \n",
    "        self.step_frame_stack(frames)\n",
    "        self.Return += reward\n",
    "        if done:\n",
    "            info[\"return\"] = self.Return\n",
    "        \n",
    "        if reward > 0:\n",
    "            reward = 1\n",
    "        elif reward == 0:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = -1\n",
    "            \n",
    "        return self.frame_stack, reward, done, info\n",
    "    \n",
    "    def step_frame_stack(self, frames):\n",
    "        num_frames = len(frames)\n",
    "        if num_frames == self.k:\n",
    "            self.frame_stack = np.stack(frames)\n",
    "        elif num_frames > self.k:\n",
    "            self.frame_stack = np.array(frames[-k::])\n",
    "        else:\n",
    "            self.frame_stack[0: self.k - num_frames] = self.frame_stack[num_frames::]\n",
    "            self.frame_stack[self.k - num_frames::] = np.array(frames)  \n",
    "            \n",
    "    def preprocess_observation(self, ob):\n",
    "        ob = cv2.cvtColor(ob[self.frame_cutout_h[0]:self.frame_cutout_h[1],self.frame_cutout_w[0]:self.frame_cutout_w[1]], cv2.COLOR_BGR2GRAY)\n",
    "        ob = cv2.resize(ob, dsize=self.dsize)\n",
    "    \n",
    "        return ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_channels, num_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        network = [\n",
    "            torch.nn.Conv2d(in_channels, 32, kernel_size=8, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*7*7,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_actions)\n",
    "        ]\n",
    "        self.network = nn.Sequential(*network)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        actions = self.network(x)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, num_actions, epsilon):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_actions = num_actions\n",
    "        self.network = DQN(in_channels, num_actions)\n",
    "        self.eps = epsilon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        actions = self.network(x)\n",
    "        return actions\n",
    "    \n",
    "    def e_greedy(self, x):\n",
    "        actions = self.forward(x)\n",
    "        greedy = torch.rand(1)\n",
    "        if self.eps < greedy:\n",
    "            return torch.argmax(actions)\n",
    "        else:\n",
    "            return (torch.rand(1) * self.num_actions).type('torch.LongTensor')[0] \n",
    "    \n",
    "    def set_epsilon(self, epsilon):\n",
    "        self.eps = epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        f = open(f\"{self.filename}.csv\", \"w\")\n",
    "        f.close()\n",
    "        \n",
    "    def log(self, msg):\n",
    "        f = open(f\"{self.filename}.csv\", \"a+\")\n",
    "        f.write(f\"{msg}\\n\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experience_Replay():\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def insert(self, transitions):\n",
    "        for i in range(len(transitions)):\n",
    "            if len(self.memory) < self.capacity:\n",
    "                self.memory.append(None)\n",
    "            self.memory[self.position] = transitions[i]\n",
    "            self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def get(self, batch_size):\n",
    "        indexes = (np.random.rand(batch_size) * (len(self.memory)-1)).astype(int)\n",
    "        return [self.memory[i] for i in indexes]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env_Runner:\n",
    "    \n",
    "    def __init__(self, env, agent):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        self.logger = Logger(\"training/training_info\")\n",
    "        self.logger.log(\"training_step, training_return\")\n",
    "        self.ob = self.env.reset()\n",
    "        self.total_steps = 0\n",
    "        \n",
    "    def run(self, steps):\n",
    "        obs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        dones = []\n",
    "        for step in range(steps):\n",
    "            self.ob = torch.tensor(self.ob)\n",
    "            action = self.agent.e_greedy(\n",
    "                self.ob.to(device).to(dtype).unsqueeze(0) / 255)\n",
    "            action = action.detach().cpu().numpy()\n",
    "            obs.append(self.ob)\n",
    "            actions.append(action)\n",
    "            self.ob, r, done, info = self.env.step(action)\n",
    "            if done:\n",
    "                self.ob = self.env.reset()\n",
    "                if \"return\" in info:\n",
    "                    self.logger.log(f'{self.total_steps+step},{info[\"return\"]}')\n",
    "            rewards.append(r)\n",
    "            dones.append(done)\n",
    "        self.total_steps += steps\n",
    "        \n",
    "        return obs, actions, rewards, dones\n",
    "    \n",
    "def make_transitions(obs, actions, rewards, dones):\n",
    "    tuples = []\n",
    "    steps = len(obs) - 1\n",
    "    for t in range(steps):\n",
    "        tuples.append((obs[t],actions[t],rewards[t],obs[t+1],int(not dones[t])))\n",
    "\n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_env = gym.make(env_name)\n",
    "env = Atari_Wrapper(raw_env, env_name, num_stacked_frames)\n",
    "\n",
    "in_channels = num_stacked_frames\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "eps_interval = start_eps-final_eps\n",
    "\n",
    "agent = Agent(in_channels, num_actions, start_eps).to(device)\n",
    "target_agent = Agent(in_channels, num_actions, start_eps).to(device)\n",
    "target_agent.load_state_dict(agent.state_dict())\n",
    "\n",
    "replay = Experience_Replay(replay_memory_size)\n",
    "runner = Env_Runner(env, agent)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=lr)\n",
    "huber_loss = torch.nn.SmoothL1Loss()\n",
    "\n",
    "num_steps = 0\n",
    "num_model_updates = 0\n",
    "\n",
    "start_time = time.time()\n",
    "while num_steps < total_steps:\n",
    "    new_epsilon = np.maximum(final_eps, start_eps - ( eps_interval * num_steps/final_eps_frame))\n",
    "    agent.set_epsilon(new_epsilon)\n",
    "    \n",
    "    obs, actions, rewards, dones = runner.run(steps_rollout)\n",
    "    transitions = make_transitions(obs, actions, rewards, dones)\n",
    "    replay.insert(transitions)\n",
    "    \n",
    "    num_steps += steps_rollout\n",
    "    \n",
    "    if num_steps < min_replay_size_to_update:\n",
    "        continue\n",
    "    \n",
    "    for update in range(4):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        minibatch = replay.get(minibatch_size)\n",
    "        \n",
    "        obs = (torch.stack([i[0] for i in minibatch]).to(device).to(dtype)) / 255 \n",
    "        \n",
    "        actions = np.stack([i[1] for i in minibatch])\n",
    "        rewards = torch.tensor([i[2] for i in minibatch]).to(device)\n",
    "        \n",
    "        next_obs = (torch.stack([i[3] for i in minibatch]).to(device).to(dtype)) / 255\n",
    "        \n",
    "        dones = torch.tensor([i[4] for i in minibatch]).to(device)\n",
    "        \n",
    "        Qs = agent(torch.cat([obs, next_obs]))\n",
    "        obs_Q, next_obs_Q = torch.split(Qs, minibatch_size ,dim=0)\n",
    "        \n",
    "        obs_Q = obs_Q[range(minibatch_size), actions]\n",
    "        \n",
    "        next_obs_Q_max = torch.max(next_obs_Q,1)[1].detach()\n",
    "        target_Q = target_agent(next_obs)[range(minibatch_size), next_obs_Q_max].detach()\n",
    "        \n",
    "        target = rewards + gamma * target_Q * dones\n",
    "        \n",
    "        loss = huber_loss(obs_Q, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    num_model_updates += 1\n",
    "     \n",
    "    if num_model_updates%target_net_update == 0:\n",
    "        target_agent.load_state_dict(agent.state_dict())\n",
    "\n",
    "    if num_steps%50000 < steps_rollout:\n",
    "        end_time = time.time()\n",
    "        print(f'##### total steps: {num_steps} | loss: {loss} #####')\n",
    "        start_time = time.time()\n",
    "    \n",
    "    if num_steps%save_model_steps < steps_rollout:\n",
    "        torch.save(agent,f\"{env_name}-{num_steps}.pt\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
